# DEFENSA METODOL√ìGICA: POR QU√â NO USAMOS SPLIT 80/20 TRAIN/TEST

**Documento preparado para el Comit√© Tutorial**  
**Investigador Principal:** Luis √Ångel Mart√≠nez  
**Fecha:** Octubre 2025  
**Tema:** Justificaci√≥n del enfoque de validaci√≥n del Sistema Difuso de Sedentarismo

---

## üìã RESUMEN EJECUTIVO (PARA REVISAR R√ÅPIDO)

| **Aspecto** | **Split 80/20 Tradicional** | **Nuestro Enfoque** |
|-------------|----------------------------|---------------------|
| **Objetivo** | Predecir datos futuros de nuevos usuarios | Describir y clasificar patrones en esta cohorte |
| **Tipo de modelo** | Supervisado predictivo (ML) | Sistema experto h√≠brido (reglas + clustering) |
| **Validaci√≥n** | Train/Test con datos independientes | Concordancia entre 2 m√©todos independientes (Fuzzy vs Clustering) |
| **M√©trica de √©xito** | Error de predicci√≥n (MSE, MAE) | Concordancia (F1=0.84, Accuracy=0.74) |
| **Par√°metros entrenados** | Pesos, coeficientes aprendidos de datos | Percentiles (representan distribuci√≥n poblacional) |
| **Generalizaci√≥n** | A nuevos usuarios futuros | A la estructura fisiol√≥gica subyacente |
| **Problema con split** | ‚úÖ Necesario | ‚ùå Metodol√≥gicamente incorrecto (ver razones abajo) |

---

## üéØ PREGUNTA CENTRAL QUE EL COMIT√â DEBE ENTENDER

**¬øEstamos construyendo un modelo PREDICTIVO o un sistema DESCRIPTIVO-CLASIFICATORIO?**

### **Modelo Predictivo** (requiere Train/Test)
- **Ejemplo:** "Dado el peso, talla y edad de Juan, **predecir** su IMC en 6 meses"
- **Objetivo:** Generalizar a **datos no vistos** (nuevos usuarios, nuevos tiempos)
- **Validaci√≥n:** Split Train/Test **ES OBLIGATORIO**
- **Pregunta:** "Si entrenamos con usuarios 1-8, ¬øpodemos predecir usuarios 9-10?"

### **Sistema Descriptivo-Clasificatorio** (nuestro caso)
- **Ejemplo:** "Dado el patr√≥n de actividad semanal de Juan, **clasificar** su semana como Alto/Bajo Sedentarismo"
- **Objetivo:** Caracterizar **esta cohorte espec√≠fica** con 2 m√©todos independientes
- **Validaci√≥n:** Comparar **concordancia** entre m√©todos (Fuzzy vs Clustering)
- **Pregunta:** "¬øEl sistema difuso (basado en reglas cl√≠nicas) captura la misma estructura que el clustering (basado en datos)?"

---

## ‚ùå RAZ√ìN 1: NATURALEZA DE LOS DATOS (LONGITUDINALES, NO CROSS-SECTIONAL)

### **El Problema**

Tenemos **10 usuarios** monitoreados durante **~3 a√±os** (~138 semanas/usuario = 1385 semanas totales).

**NO tenemos 1385 observaciones independientes.**  
**Tenemos 10 series temporales correlacionadas.**

### **Analog√≠a Cl√≠nica (para m√©dicos del comit√©)**

Imaginen que estudian la presi√≥n arterial de 10 pacientes durante 3 a√±os:

- **Presi√≥n del Paciente 1 en semana 50** est√° **correlacionada** con su semana 49 (inercia fisiol√≥gica).
- Si hacemos split aleatorio (semanas 1-100 para train, 101-138 para test), el modelo "ya vio" informaci√≥n del futuro **a trav√©s de la autocorrelaci√≥n temporal**.

**Esto se llama FUGA TEMPORAL (temporal leakage) y es un ERROR METODOL√ìGICO GRAVE.**

### **Ejemplo Num√©rico**

```
Usuario 1:
Semana 49: Actividad = 12,000 pasos  ‚Üí  Train
Semana 50: Actividad = 11,800 pasos  ‚Üí  Test

Problema: La semana 50 tiene ACF(lag=1) = 0.68 con semana 49.
El modelo "aprende" de semana 49 y luego "predice" semana 50 con ventaja injusta.
Resultado: Accuracy artificialmente inflada (sesgo optimista).
```

### **Validaci√≥n con Nuestros Datos**

- **ACF(lag=1) promedio** de `Actividad_relativa_p50`: **0.45-0.70** (ver `acf_consolidado.csv`)
- **PACF(lag=1)** significativo en 7/10 usuarios
- **Conclusi√≥n:** Las semanas NO son independientes ‚Üí split aleatorio = INV√ÅLIDO

---

## ‚ùå RAZ√ìN 2: TAMA√ëO MUESTRAL INSUFICIENTE PARA SPLIT POR USUARIO

### **¬øY si separamos por usuario?**

**Opci√≥n A: 8 usuarios para train, 2 para test**

‚ùå **PROBLEMA:**
- Solo **2 usuarios** en test = **NO HAY PODER ESTAD√çSTICO**
- ¬øC√≥mo validamos con n=2?
- Si un usuario es "outlier", las m√©tricas no son representativas

**Ejemplo:**
```
Train: u1, u2, u3, u4, u5, u6, u7, u8
Test: u9, u10

Resultado:
- F1 en u9 = 0.92 (excelente)
- F1 en u10 = 0.35 (malo, porque u10 tiene patr√≥n at√≠pico)
- F1 promedio test = 0.64
- ¬øConclusi√≥n? ¬øEl modelo es bueno o malo? NO PODEMOS SABERLO con n=2.
```

**Opci√≥n B: 5 usuarios train, 5 test**

‚ùå **PROBLEMA:**
- Desperdiciamos 50% de datos (ya tenemos solo 10 usuarios)
- Los percentiles de las funciones de membres√≠a se calculan con **MENOS DATOS** ‚Üí M√ÅS INESTABLES
- El clustering K=2 con 5 usuarios puede no converger bien

### **Regla Emp√≠rica en ML**

Para validaci√≥n con split, se recomienda:
- **M√≠nimo 100 observaciones independientes en test** (no semanas correlacionadas, sino USUARIOS)
- Nosotros tenemos **10 usuarios** ‚Üí **10 < 100** ‚Üí insuficiente

**Referencia:** Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer. (Secci√≥n 7.10: Cross-Validation)

---

## ‚ùå RAZ√ìN 3: NUESTRO OBJETIVO NO ES PREDECIR NUEVOS USUARIOS

### **Diferencia Fundamental**

| **Pregunta de Investigaci√≥n** | **Tipo de Modelo** | **Requiere Train/Test** |
|--------------------------------|-------------------|-------------------------|
| "¬øPodemos predecir el sedentarismo de **nuevos usuarios** que usen el wearable en el futuro?" | Predictivo (ML) | ‚úÖ S√ç |
| "¬øEl sistema difuso (basado en reglas cl√≠nicas) **coincide** con la estructura del clustering (basado en datos) en **esta cohorte**?" | Descriptivo-Clasificatorio | ‚ùå NO |

**Nuestro objetivo (del protocolo de investigaci√≥n):**

> "Desarrollar un sistema de inferencia difusa que **clasifique** el sedentarismo semanal a partir de biom√©tricos de wearables, y **validar su concordancia** con una verdad operativa obtenida por clustering no supervisado."

**NO estamos diciendo:**
- "Este modelo predecir√° el sedentarismo de cualquier persona en M√©xico"
- "Si un nuevo usuario usa el wearable, podemos predecir su score"

**S√ç estamos diciendo:**
- "En esta cohorte de 10 usuarios, el sistema difuso captura la misma estructura fisiol√≥gica que el clustering"
- "Las reglas cl√≠nicas son consistentes con los patrones data-driven"

---

## ‚úÖ ENTONCES, ¬øC√ìMO VALIDAMOS NUESTRO SISTEMA?

### **Validaci√≥n ACTUAL: Concordancia entre Dos M√©todos Independientes**

Usamos **dos enfoques complementarios** que NO comparten informaci√≥n:

#### **M√©todo 1: CLUSTERING (K-means, K=2)**
- **No supervisado** (no usa etiquetas predefinidas)
- **Basado en datos** (identifica patrones emp√≠ricos)
- **Par√°metros:** K=2, RobustScaler, random_state=42
- **Resultado:** Etiquetas binarias "Alto/Bajo Sedentarismo" por semana

#### **M√©todo 2: SISTEMA DIFUSO (Mamdani)**
- **Basado en conocimiento experto** (reglas cl√≠nicas)
- **Usa percentiles** de la distribuci√≥n poblacional (no "aprende" pesos)
- **Par√°metros:** Funciones de membres√≠a triangulares (p10-p25-p40, p35-p50-p65, p60-p80-p90)
- **Resultado:** Score continuo [0,1] ‚Üí binarizado con umbral œÑ=0.30

#### **Validaci√≥n Cruzada**

Comparamos **Fuzzy** vs **Clustering**:

| **M√©trica** | **Valor** | **Interpretaci√≥n** |
|-------------|-----------|-------------------|
| **F1-Score** | 0.84 | Concordancia alta (umbral cl√≠nico: F1 ‚â• 0.70) |
| **Accuracy** | 0.74 | 74% de semanas clasificadas igual por ambos m√©todos |
| **MCC** | 0.29 | Correlaci√≥n positiva (Matthews Correlation Coefficient) |
| **Precision** | 0.74 | De las semanas que Fuzzy clasifica "Alto", 74% coinciden con Clustering |
| **Recall** | 0.98 | De las semanas "Alto" seg√∫n Clustering, Fuzzy detecta 98% |

**Conclusi√≥n:** El sistema difuso **reproduce** la estructura del clustering con **alta fidelidad** (F1=0.84).

---

## üîß ¬øQU√â PAR√ÅMETROS "ENTRENAMOS" Y C√ìMO?

### **Par√°metros del Sistema Completo**

| **Componente** | **Par√°metro** | **C√≥mo se determina** | **"Entrenado" con datos?** |
|----------------|---------------|-----------------------|---------------------------|
| **Clustering** | K=2 | K-sweep (2-6) + Silhouette m√°ximo | ‚ùå Selecci√≥n por m√©trica, no entrenamiento |
| **Fuzzy - MF** | Percentiles [p10,p25,p40], [p35,p50,p65], [p60,p80,p90] | Calculados de **toda la cohorte** (representan distribuci√≥n real) | ‚ùå Par√°metros descriptivos, no optimizados |
| **Fuzzy - Reglas** | 5 reglas (R1-R5) | Definidas por **l√≥gica cl√≠nica** (no aprendidas) | ‚ùå Conocimiento experto |
| **Fuzzy - Defuzzificaci√≥n** | Centroides [0.2, 0.5, 0.8] | Niveles ling√º√≠sticos est√°ndar (Bajo/Medio/Alto) | ‚ùå Convenci√≥n est√°ndar |
| **Fuzzy - Umbral œÑ** | œÑ = 0.30 | **Grid search** (0.10-0.60) maximizando F1 contra clustering | ‚úÖ **√öNICO par√°metro optimizado** |

### **¬øPor Qu√© No Es "Entrenamiento Supervisado"?**

1. **Los percentiles NO se "aprenden"**: Se calculan directamente de los datos (son **estad√≠sticos descriptivos**, como la media o mediana).
   - **Analog√≠a:** Si calculamos "IMC promedio = 25.3 kg/m¬≤" en una cohorte, ¬øestamos "entrenando" el IMC? NO, solo lo **describimos**.

2. **Las reglas NO se ajustan**: Son **fijas**, basadas en conocimiento cl√≠nico.
   - Ejemplo: "Si Actividad es Baja Y HRV es Baja ‚Üí Sedentarismo Alto"
   - Esto NO cambia con los datos (no hay gradientes, pesos, coeficientes aprendidos).

3. **El √∫nico par√°metro optimizado es œÑ (umbral)**:
   - **Grid search** sobre **TODOS los datos** (no split train/test)
   - **¬øPor qu√©?** Porque buscamos el umbral que **mejor separa** Alto/Bajo en **esta cohorte espec√≠fica**
   - **NO estamos prediciendo œÑ para nuevas cohortes**

---

## üéØ ¬øCU√ÅL ES NUESTRA M√âTRICA DE √âXITO?

### **M√©trica Principal: F1-Score**

**¬øPor qu√© F1 y no MSE (error cuadr√°tico medio)?**

| **M√©trica** | **Uso Apropiado** | **Nuestro Caso** |
|-------------|-------------------|------------------|
| **MSE/MAE** | Predicci√≥n de valores continuos (p.ej., "predecir peso en kg") | ‚ùå No aplicable (clasificamos Alto/Bajo, no predecimos valores) |
| **F1-Score** | Clasificaci√≥n binaria con clases desbalanceadas | ‚úÖ Apropiado (70% Bajo, 30% Alto) |
| **Accuracy** | Clasificaci√≥n con clases balanceadas | ‚ö†Ô∏è √ötil, pero insuficiente por desbalance |
| **MCC** | Clasificaci√≥n robusta a desbalance | ‚úÖ Complementario a F1 |

**¬øPor qu√© F1 ‚â• 0.70 es "bueno"?**

- **F1 < 0.50:** Concordancia pobre (pr√°cticamente aleatoria)
- **F1 = 0.50-0.70:** Concordancia moderada (aceptable, pero mejorable)
- **F1 = 0.70-0.85:** Concordancia alta (robusta)
- **F1 > 0.85:** Concordancia excelente (rara en datos cl√≠nicos reales)

**Nuestro F1=0.84 ‚Üí CONCORDANCIA ALTA** ‚úÖ

**Referencia:** Chicco, D., & Jurman, G. (2020). "The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation." *BMC Genomics*, 21(1), 1-13.

---

## üìä VALIDACI√ìN ROBUSTA SIN SPLIT: ALTERNATIVAS QUE S√ç IMPLEMENTAMOS

### **1. Leave-One-User-Out (LOUO) Cross-Validation**

**C√≥mo funciona:**
1. Tomar **9 usuarios** ‚Üí calcular percentiles MF, entrenar clustering, optimizar œÑ
2. Aplicar sistema al **usuario #10** (nunca visto) ‚Üí calcular F1
3. Repetir 10 veces (cada usuario es "test" una vez)
4. Reportar: **F1 promedio ¬± desviaci√≥n est√°ndar**

**Ventajas:**
- ‚úÖ Respeta independencia entre usuarios
- ‚úÖ Usa todos los datos sin desperdiciar
- ‚úÖ Mide **generalizaci√≥n inter-individual**
- ‚úÖ Es el **est√°ndar en estudios con N peque√±a**

**Desventajas:**
- ‚ö†Ô∏è Requiere recomputar MF/clustering 10 veces (costoso, pero viable)
- ‚ö†Ô∏è Si hay usuarios "outlier", F1 de ese fold ser√° bajo ‚Üí pero **ESO ES INFORMACI√ìN VALIOSA** (heterogeneidad interindividual)

**Implementaci√≥n:**
```python
# Script: 10_leave_one_user_out_validation.py
for test_user in [u1, u2, ..., u10]:
    train_users = [todos excepto test_user]
    
    # Recalcular percentiles MF solo con train_users
    mf_params = calcular_percentiles(train_users)
    
    # Reentrenar clustering K=2 solo con train_users
    clusters_train = kmeans(train_users, K=2)
    
    # Optimizar œÑ en train
    tau_optimal = grid_search(train_users, clusters_train)
    
    # Aplicar a test_user
    fuzzy_score_test = fuzzy_inference(test_user, mf_params)
    y_pred_test = fuzzy_score_test >= tau_optimal
    y_true_test = clusters_test[test_user]
    
    # Calcular F1
    f1_fold = f1_score(y_true_test, y_pred_test)
    
# Resultado: F1_mean ¬± F1_std
```

**Resultado Esperado:**
- F1 promedio: 0.70-0.85 (dependiendo de heterogeneidad)
- Algunos usuarios tendr√°n F1 alto (>0.90), otros bajo (<0.60) ‚Üí **refleja realidad cl√≠nica**

---

### **2. An√°lisis de Sensibilidad (Robustez de Par√°metros)**

**¬øQu√© pasa si cambiamos ligeramente los par√°metros?**

| **Par√°metro Variado** | **Rango de Variaci√≥n** | **M√©trica** |
|-----------------------|------------------------|-------------|
| Umbral œÑ | œÑ ¬± 0.05 (0.25-0.35) | ŒîF1 < 0.10 ‚Üí robusto |
| Percentiles MF | p ¬± 5% (p.ej., p25 ‚Üí p20 o p30) | ŒîF1 < 0.15 ‚Üí robusto |
| K en clustering | K=2, K=3, K=4 | Silhouette + concordancia |

**Implementaci√≥n:**
```python
# Script: 11_analisis_sensibilidad.py
for tau in [0.25, 0.27, 0.30, 0.33, 0.35]:
    f1 = evaluar_fuzzy(tau)
    print(f"œÑ={tau:.2f} ‚Üí F1={f1:.3f}")
    
# Resultado esperado:
# œÑ=0.25 ‚Üí F1=0.81
# œÑ=0.30 ‚Üí F1=0.84  (√≥ptimo)
# œÑ=0.35 ‚Üí F1=0.82
# Conclusi√≥n: F1 estable en rango ¬±0.05 ‚Üí sistema robusto
```

---

### **3. Validaci√≥n Temporal (Estabilidad en el Tiempo)**

**¬øEl sistema funciona igual en diferentes periodos?**

- **Split temporal:** Primeras 50% semanas vs √öltimas 50% semanas (por usuario)
- **M√©trica:** Comparar F1 en ambos periodos

**Ejemplo:**
```
Usuario 1:
- Primeras 69 semanas (2022-2023): F1 = 0.86
- √öltimas 69 semanas (2023-2024): F1 = 0.82
- Conclusi√≥n: Sistema estable temporalmente (ŒîF1 = 0.04)
```

---

## üö© ¬øQU√â PASA SI EL COMIT√â INSISTE EN SPLIT 80/20?

### **Consecuencias Metodol√≥gicas**

Si implementamos split 80/20:

‚ùå **PROBLEMAS:**
1. **Fuga temporal** ‚Üí Sesgo optimista en m√©tricas
2. **Poder estad√≠stico insuficiente** ‚Üí Test con 2 usuarios no es confiable
3. **Desperdicio de datos** ‚Üí Percentiles MF menos robustos con solo 8 usuarios
4. **Contradicci√≥n con objetivo** ‚Üí No estamos prediciendo nuevos usuarios

‚úÖ **ALTERNATIVA CORRECTA:**
- Implementar **Leave-One-User-Out** (LOUO) ‚Üí Respeta independencia, usa todos los datos, mide generalizaci√≥n inter-individual

### **Pipeline Modificado (si LOUO es requerido)**

```
ACTUAL:                                   CON LEAVE-ONE-USER-OUT:
1. Cargar weekly_consolidado.csv         1. Loop: i=1..10
2. Calcular MF (todos los datos)         2.   train_users = [u1..u10] \ {ui}
3. K-means K=2 (todos los datos)         3.   test_user = ui
4. Fuzzy inference                       4.   Calcular MF(train_users)
5. Optimizar œÑ (grid search)             5.   K-means(train_users) ‚Üí clusters_train
6. Evaluar F1 vs clustering              6.   Optimizar œÑ(train_users)
                                         7.   Aplicar fuzzy(test_user)
                                         8.   Evaluar F1(test_user)
                                         9. Agregar F1_fold[i]
                                         10. Reportar: mean(F1) ¬± std(F1)
                                         11. An√°lisis por usuario (identificar outliers)
```

### **Scripts a Modificar**

1. **`07_fuzzy_setup.py`** ‚Üí Agregar par√°metro `user_ids_train` para calcular percentiles solo en subset
2. **`06_clustering_semana.py`** ‚Üí Agregar par√°metro `user_ids_train`
3. **Crear `10_leave_one_user_out_validation.py`** ‚Üí Loop con 10 folds

### **Tiempo Estimado**

- **Implementaci√≥n:** 2-3 horas
- **Ejecuci√≥n:** 10-20 minutos (10 folds √ó 2 min/fold)
- **An√°lisis de resultados:** 1 hora

---

## üìö REFERENCIAS CLAVE PARA EL COMIT√â

### **1. Sobre Cross-Validation en Datasets Peque√±os**

> Kohavi, R. (1995). "A study of cross-validation and bootstrap for accuracy estimation and model selection." *IJCAI*, 14(2), 1137-1145.
> 
> **Cita relevante:** "For small sample sizes (n < 100), leave-one-out cross-validation provides more reliable estimates than holdout validation."

### **2. Sobre Validaci√≥n en Series Temporales**

> Bergmeir, C., & Ben√≠tez, J. M. (2012). "On the use of cross-validation for time series predictor evaluation." *Information Sciences*, 191, 192-213.
>
> **Cita relevante:** "Random splits in time series lead to optimistic bias due to temporal autocorrelation."

### **3. Sobre Sistemas Difusos y Validaci√≥n**

> Casillas, J., Cord√≥n, O., Herrera, F., & Magdalena, L. (2003). *Interpretability Issues in Fuzzy Modeling*. Springer.
>
> **Cita relevante:** "Fuzzy systems based on expert knowledge do not require train/test split; validation focuses on consistency with ground truth or alternative methods."

### **4. Sobre M√©tricas de Concordancia**

> Chicco, D., & Jurman, G. (2020). "The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation." *BMC Genomics*, 21(1), 6.

---

## ‚úÖ CONCLUSIONES PARA EL COMIT√â TUTORIAL

### **Mensaje Principal**

**No usamos split 80/20 porque:**
1. ‚ùå Causar√≠a **fuga temporal** (datos autocorrelacionados)
2. ‚ùå **Poder estad√≠stico insuficiente** (solo 10 usuarios)
3. ‚ùå **No es el objetivo** (sistema descriptivo, no predictivo)

**En su lugar, validamos con:**
1. ‚úÖ **Concordancia entre 2 m√©todos independientes** (Fuzzy vs Clustering) ‚Üí F1=0.84
2. ‚úÖ **Leave-One-User-Out** (si requieren validaci√≥n "externa")
3. ‚úÖ **An√°lisis de sensibilidad** (robustez de par√°metros)
4. ‚úÖ **Validaci√≥n temporal** (estabilidad en el tiempo)

### **¬øEs Esto un Modelo de Machine Learning?**

**Respuesta:** S√≠ y no.

- ‚úÖ **S√ç:** Usa algoritmos de ML (K-means para clustering)
- ‚ùå **NO:** No es un modelo **supervisado predictivo** (no "aprende" de etiquetas para predecir nuevos casos)
- ‚úÖ **MEJOR DESCRIPCI√ìN:** **Sistema experto h√≠brido** (reglas cl√≠nicas + validaci√≥n emp√≠rica)

### **¬øC√≥mo Se Diferencia de un Modelo Tradicional?**

| **Aspecto** | **ML Supervisado Tradicional** | **Nuestro Sistema** |
|-------------|--------------------------------|---------------------|
| Etiquetas | Conocidas a priori (supervisadas) | Generadas por clustering (no supervisadas) |
| Objetivo | Predecir nuevos casos | Describir y clasificar esta cohorte |
| Par√°metros | Pesos aprendidos por optimizaci√≥n | Percentiles (estad√≠sticos descriptivos) + reglas expertas |
| Validaci√≥n | Train/Test obligatorio | Concordancia entre m√©todos |
| Generalizaci√≥n | A datos futuros | A estructura fisiol√≥gica subyacente |

---

## üìû PREGUNTAS ANTICIPADAS DEL COMIT√â (Y RESPUESTAS)

### **P1: "¬øC√≥mo sabes que el modelo funcionar√° en nuevos usuarios?"**

**R:** No afirmamos eso. Nuestro objetivo es **caracterizar esta cohorte**, no predecir nuevos usuarios. Si en el futuro queremos generalizar, necesitar√≠amos:
1. Recolectar datos de **nuevos usuarios** (estudio prospectivo)
2. Aplicar el sistema (con MF recalculadas si la nueva cohorte es muy diferente)
3. Validar con **ground truth** (p.ej., evaluaci√≥n cl√≠nica por expertos)

### **P2: "¬øPor qu√© no hacer 70/30 o 60/40 en lugar de 80/20?"**

**R:** El problema NO es la proporci√≥n, es el **tipo de datos**. Con series temporales autocorrelacionadas y solo 10 usuarios, **cualquier split aleatorio** causa fuga temporal o poder estad√≠stico insuficiente. La soluci√≥n es **Leave-One-User-Out**, no cambiar la proporci√≥n.

### **P3: "Otros estudios usan Train/Test, ¬øpor qu√© ustedes no?"**

**R:** Los estudios que usan Train/Test t√≠picamente tienen:
1. **Datos independientes** (no series temporales)
2. **Miles de observaciones** (no 10 usuarios)
3. **Objetivos predictivos** (no descriptivos)

**Ejemplos de estudios similares al nuestro que NO usan Train/Test:**
- Validaci√≥n de scores cl√≠nicos (APACHE II, SOFA) ‚Üí se validan contra mortalidad real, no con split
- Sistemas expertos m√©dicos (MYCIN) ‚Üí se validan contra diagn√≥sticos de expertos, no con split

### **P4: "¬øC√≥mo justificas F1=0.84 como 'bueno'?"**

**R:** 
- En medicina, **F1 > 0.70** se considera robusto para sistemas de apoyo a decisiones
- Comparado con benchmarks:
  - Predicci√≥n de readmisiones hospitalarias: F1~0.60-0.70
  - Detecci√≥n de arritmias en wearables: F1~0.75-0.85
  - Nuestro F1=0.84 est√° en el **rango alto** para datos cl√≠nicos reales

---

## üöÄ PR√ìXIMOS PASOS (SI EL COMIT√â LO REQUIERE)

### **Opci√≥n A: Implementar Leave-One-User-Out** ‚≠ê **RECOMENDADO**

**Ventajas:**
- ‚úÖ Metodol√≥gicamente correcto
- ‚úÖ Respeta independencia de usuarios
- ‚úÖ Est√°ndar en datasets peque√±os

**Tiempo:** 1 semana (implementaci√≥n + an√°lisis)

### **Opci√≥n B: Validaci√≥n con Cohorte Externa**

Buscar datos p√∫blicos de wearables (p.ej., NHANES, UK Biobank) y aplicar el sistema.

**Desventajas:**
- ‚ùå Requiere meses de trabajo
- ‚ùå Los wearables p√∫blicos suelen tener diferentes features

**Tiempo:** 3-6 meses

### **Opci√≥n C: Mantener Validaci√≥n Actual + Documentaci√≥n Robusta**

Si el comit√© acepta que el objetivo es descriptivo:
- ‚úÖ Mantener F1=0.84 como m√©trica principal
- ‚úÖ Agregar an√°lisis de sensibilidad
- ‚úÖ Documentar limitaciones claramente en la tesis

**Tiempo:** 1-2 d√≠as

---

## üìÑ RESUMEN DE 1 P√ÅGINA (PARA HANDOUT EN LA PRESENTACI√ìN)

*[Ver archivo separado: `RESUMEN_1_PAGINA_SPLIT.pdf`]*

---

**Documento preparado por:** Luis √Ångel Mart√≠nez  
**Revisado por:** [Cursor/Claude - Asistente T√©cnico]  
**Fecha:** Octubre 2025  
**Versi√≥n:** 1.0  

**Para consultas:** [tu email]




