================================================================================
PSEUDOC√ìDIGO COMPLETO DEL PIPELINE
Sistema Difuso para Clasificaci√≥n de Sedentarismo
================================================================================

AUTOR: Luis √Ångel Mart√≠nez
FECHA: 2025-10-18
VERSI√ìN: 1.0

================================================================================
√çNDICE
================================================================================
1. FASE 1: CARGA Y LIMPIEZA DE DATOS
2. FASE 2: AGREGACI√ìN SEMANAL
3. FASE 3: FEATURE ENGINEERING
4. FASE 4: AN√ÅLISIS EXPLORATORIO (EDA)
5. FASE 5: CLUSTERING (VERDAD OPERATIVA)
6. FASE 6: SISTEMA DIFUSO
7. FASE 7: VALIDACI√ìN PRIMARIA
8. FASE 8: AN√ÅLISIS DE ROBUSTEZ
9. FASE 9: FORMALIZACI√ìN MATEM√ÅTICA
10. FASE 10: GENERACI√ìN DE REPORTES

================================================================================
CONFIGURACI√ìN GLOBAL
================================================================================

CONSTANTS:
    N_USUARIOS = 10
    USUARIOS = ['u1', 'u2', ..., 'u10']
    NOMBRES = ['ale', 'brenda', 'christina', ..., 'vane']
    
    COLUMNAS_RAW = [
        'date', 'steps', 'distance_km', 'calories',
        'FC_rest', 'FC_walk', 'HRV_SDNN',
        'age', 'weight', 'height', 'TMB'
    ]
    
    FEATURES_CLUSTER = [
        'Actividad_relativa_p50', 'Actividad_relativa_iqr',
        'Superavit_calorico_basal_p50', 'Superavit_calorico_basal_iqr',
        'HRV_SDNN_p50', 'HRV_SDNN_iqr',
        'Delta_cardiaco_p50', 'Delta_cardiaco_iqr'
    ]
    
    FEATURES_FUZZY = [
        'Actividad_relativa_p50',
        'Superavit_calorico_basal_p50',
        'HRV_SDNN_p50',
        'Delta_cardiaco_p50'
    ]
    
    PERCENTILES_MF = {
        'Baja': [10, 25, 40],
        'Media': [35, 50, 65],
        'Alta': [60, 80, 90]
    }
    
    TAU_UMBRAL = 0.30
    K_CLUSTERS = 2
    RANDOM_STATE = 42

================================================================================
FASE 1: CARGA Y LIMPIEZA DE DATOS
================================================================================

FUNCTION cargar_datos_usuario(usuario_id):
    """
    Carga y limpia datos crudos de un usuario.
    
    Input: usuario_id (str) - Identificador del usuario (u1...u10)
    Output: DataFrame limpio
    """
    
    # 1. Cargar archivo CSV
    filepath = f"DB_final_v3_{usuario_id}.csv"
    df = READ_CSV(filepath)
    
    # 2. Validar columnas
    FOR col IN COLUMNAS_RAW:
        IF col NOT IN df.columns:
            RAISE ERROR("Columna faltante: {col}")
    
    # 3. Convertir fecha a datetime
    df['date'] = TO_DATETIME(df['date'])
    
    # 4. Limpiar valores inv√°lidos
    FOR col IN df.columns:
        IF col != 'date':
            # Reemplazar '-', '--', 'NaN', '' con NaN
            df[col] = REPLACE(df[col], ['-', '--', 'NaN', ''], NaN)
            
            # Convertir a num√©rico
            df[col] = TO_NUMERIC(df[col])
    
    # 5. Detectar y remover outliers (percentil 5-95)
    FOR col IN ['steps', 'distance_km', 'calories', 'FC_rest', 'FC_walk', 'HRV_SDNN']:
        p5 = PERCENTILE(df[col], 5)
        p95 = PERCENTILE(df[col], 95)
        df[col] = CLIP(df[col], p5, p95)
    
    # 6. Imputaci√≥n jer√°rquica
    # a) Forward-fill (propagar √∫ltimo valor v√°lido)
    df = FORWARD_FILL(df)
    
    # b) Backward-fill (propagar siguiente valor v√°lido)
    df = BACKWARD_FILL(df)
    
    # c) Mediana global (solo si a√∫n quedan NaN)
    FOR col IN df.columns:
        IF col != 'date':
            mediana = MEDIAN(df[col], ignore_nan=True)
            df[col] = FILL_NA(df[col], mediana)
    
    RETURN df


FUNCTION cargar_todos_usuarios():
    """
    Carga datos de todos los usuarios.
    
    Output: Dict{usuario_id: DataFrame}
    """
    
    datos = {}
    
    FOR usuario_id IN USUARIOS:
        PRINT(f"Cargando {usuario_id}...")
        datos[usuario_id] = cargar_datos_usuario(usuario_id)
        PRINT(f"  ‚úÖ {usuario_id}: {LEN(datos[usuario_id])} d√≠as")
    
    RETURN datos


================================================================================
FASE 2: AGREGACI√ìN SEMANAL
================================================================================

FUNCTION agregar_semana(df_diario):
    """
    Agrega datos diarios a nivel semanal.
    
    Input: df_diario (DataFrame) - Datos diarios de un usuario
    Output: DataFrame semanal con p50 e IQR
    """
    
    # 1. Agregar columna de semana ISO (lunes = inicio)
    df_diario['semana_inicio'] = GET_ISO_WEEK_START(df_diario['date'])
    
    # 2. Agrupar por semana
    semanas_data = []
    
    FOR semana IN UNIQUE(df_diario['semana_inicio']):
        # Filtrar datos de la semana
        df_semana = df_diario[df_diario['semana_inicio'] == semana]
        
        # Contar d√≠as monitoreados
        dias_monitoreados = COUNT_NON_NAN(df_semana['steps'])
        
        # Filtrar semanas con <4 d√≠as de datos
        IF dias_monitoreados < 4:
            CONTINUE
        
        # Calcular estad√≠sticos para cada variable
        row = {
            'semana_inicio': semana,
            'dias_monitoreados': dias_monitoreados
        }
        
        FOR col IN ['steps', 'distance_km', 'calories', 'FC_rest', 'FC_walk', 'HRV_SDNN']:
            # Mediana (p50)
            row[f"{col}_p50"] = MEDIAN(df_semana[col])
            
            # IQR (dispersi√≥n robusta)
            p25 = PERCENTILE(df_semana[col], 25)
            p75 = PERCENTILE(df_semana[col], 75)
            row[f"{col}_iqr"] = p75 - p25
        
        # Agregar variables antropom√©tricas (promedios)
        FOR col IN ['age', 'weight', 'height', 'TMB']:
            row[col] = MEAN(df_semana[col])
        
        APPEND(semanas_data, row)
    
    df_semanal = TO_DATAFRAME(semanas_data)
    
    RETURN df_semanal


FUNCTION agregar_todos_usuarios(datos):
    """
    Agrega datos semanales de todos los usuarios.
    
    Input: datos (Dict{usuario_id: DataFrame diario})
    Output: DataFrame consolidado semanal
    """
    
    lista_semanales = []
    
    FOR usuario_id, df_diario IN datos.items():
        PRINT(f"Agregando {usuario_id}...")
        
        df_semanal = agregar_semana(df_diario)
        df_semanal['usuario_id'] = usuario_id
        
        APPEND(lista_semanales, df_semanal)
        PRINT(f"  ‚úÖ {usuario_id}: {LEN(df_semanal)} semanas")
    
    # Consolidar todos en un solo DataFrame
    df_consolidado = CONCATENATE(lista_semanales)
    
    # Ordenar por usuario y fecha
    df_consolidado = SORT(df_consolidado, by=['usuario_id', 'semana_inicio'])
    
    RETURN df_consolidado


================================================================================
FASE 3: FEATURE ENGINEERING
================================================================================

FUNCTION calcular_features(df):
    """
    Calcula features derivadas cl√≠nicamente relevantes.
    
    Input: df (DataFrame semanal consolidado)
    Output: DataFrame con features adicionales
    """
    
    # 1. Actividad Relativa (pasos/km)
    # Corrige exposici√≥n al wearable
    df['Actividad_relativa_p50'] = df['steps_p50'] / (df['distance_km_p50'] * 1000)
    df['Actividad_relativa_iqr'] = df['steps_iqr'] / (df['distance_km_iqr'] * 1000)
    
    # 2. Super√°vit Cal√≥rico Basal (gasto/TMB)
    # Ratio de gasto cal√≥rico sobre metabolismo basal
    df['Superavit_calorico_basal_p50'] = df['calories_p50'] / df['TMB']
    df['Superavit_calorico_basal_iqr'] = df['calories_iqr'] / df['TMB']
    
    # 3. Delta Card√≠aco (FC_walk - FC_rest)
    # Respuesta cardiovascular al ejercicio
    df['Delta_cardiaco_p50'] = df['FC_walk_p50'] - df['FC_rest_p50']
    df['Delta_cardiaco_iqr'] = df['FC_walk_iqr'] + df['FC_rest_iqr']  # suma de variabilidades
    
    # 4. HRV (ya calculado, solo renombrar para consistencia)
    # Nota: mayor HRV = mejor estado
    
    # 5. Remover infinitos y NaNs generados por divisiones
    FOR col IN FEATURES_CLUSTER:
        df[col] = REPLACE_INF(df[col], NaN)
        df[col] = FILL_NA(df[col], MEDIAN(df[col]))
    
    RETURN df


================================================================================
FASE 4: AN√ÅLISIS EXPLORATORIO (EDA)
================================================================================

FUNCTION analisis_variabilidad(df):
    """
    Analiza variabilidad intra e inter usuarios.
    
    Input: df (DataFrame con features)
    Output: DataFrame con estad√≠sticos de variabilidad
    """
    
    resultados = []
    
    # Por usuario
    FOR usuario_id IN UNIQUE(df['usuario_id']):
        df_user = df[df['usuario_id'] == usuario_id]
        
        FOR feat IN FEATURES_CLUSTER:
            # CV intra-usuario (variabilidad observada)
            media = MEAN(df_user[feat])
            std = STD(df_user[feat])
            cv = std / media IF media > 0 ELSE 0
            
            resultados.APPEND({
                'usuario_id': usuario_id,
                'variable': feat,
                'cv_intra': cv,
                'media': media,
                'std': std
            })
    
    # Inter-usuarios (variabilidad operativa)
    medianas_por_usuario = GROUP_BY(df, 'usuario_id').MEDIAN()
    
    FOR feat IN FEATURES_CLUSTER:
        media_medianas = MEAN(medianas_por_usuario[feat])
        std_medianas = STD(medianas_por_usuario[feat])
        cv_inter = std_medianas / media_medianas IF media_medianas > 0 ELSE 0
        
        resultados.APPEND({
            'usuario_id': 'CONSOLIDADO',
            'variable': feat,
            'cv_inter': cv_inter,
            'media': media_medianas,
            'std': std_medianas
        })
    
    RETURN TO_DATAFRAME(resultados)


FUNCTION analisis_correlaciones(df):
    """
    Calcula matriz de correlaci√≥n entre features.
    
    Input: df (DataFrame con features)
    Output: Matriz de correlaci√≥n (DataFrame)
    """
    
    # Seleccionar solo features num√©ricas
    df_features = df[FEATURES_CLUSTER]
    
    # Calcular correlaci√≥n de Pearson
    corr_matrix = CORRELATION(df_features, method='pearson')
    
    RETURN corr_matrix


FUNCTION analisis_missingness_acf(df):
    """
    Analiza datos faltantes y autocorrelaci√≥n.
    
    Input: df (DataFrame con features)
    Output: Dict con estad√≠sticos de missingness y ACF
    """
    
    resultados = {}
    
    FOR usuario_id IN UNIQUE(df['usuario_id']):
        df_user = df[df['usuario_id'] == usuario_id]
        
        # Missingness
        missingness = {
            'dias_monitoreados_media': MEAN(df_user['dias_monitoreados']),
            'semanas_baja_cobertura': COUNT(df_user['dias_monitoreados'] < 4) / LEN(df_user)
        }
        
        # ACF/PACF (solo si hay >30 semanas)
        IF LEN(df_user) >= 30:
            FOR feat IN FEATURES_FUZZY:
                serie = df_user[feat].values
                
                # Autocorrelaci√≥n (lag 1-10)
                acf_values = COMPUTE_ACF(serie, max_lag=10)
                
                # Autocorrelaci√≥n parcial
                pacf_values = COMPUTE_PACF(serie, max_lag=10)
                
                missingness[f'acf_{feat}'] = acf_values
                missingness[f'pacf_{feat}'] = pacf_values
        
        resultados[usuario_id] = missingness
    
    RETURN resultados


================================================================================
FASE 5: CLUSTERING (VERDAD OPERATIVA)
================================================================================

FUNCTION clustering_kmeans(df, k=2):
    """
    Aplica K-Means clustering para obtener verdad operativa.
    
    Input: df (DataFrame con features), k (n√∫mero de clusters)
    Output: df con columna 'cluster', centroides, scaler
    """
    
    # 1. Preparar datos (solo features de clustering)
    X = df[FEATURES_CLUSTER].values
    
    # 2. Escalado robusto (mediana/IQR)
    scaler = RobustScaler()
    X_scaled = scaler.FIT_TRANSFORM(X)
    
    # 3. K-Means
    kmeans = KMeans(
        n_clusters=k,
        random_state=RANDOM_STATE,
        n_init=10,
        max_iter=500
    )
    labels = kmeans.FIT_PREDICT(X_scaled)
    
    # 4. Mapeo sem√°ntico
    # Determinar cu√°l cluster corresponde a "Sedentarismo Alto"
    # Cluster con MENOR Actividad_relativa_p50 ‚Üí Alto
    df_temp = df.copy()
    df_temp['cluster_temp'] = labels
    
    cluster_means = GROUP_BY(df_temp, 'cluster_temp')['Actividad_relativa_p50'].MEAN()
    cluster_alto = ARGMIN(cluster_means)  # √çndice del cluster con menor actividad
    
    # Mapear: cluster_alto ‚Üí 1, otro ‚Üí 0
    labels_mapped = [1 IF label == cluster_alto ELSE 0 FOR label IN labels]
    
    # 5. Agregar columna al DataFrame
    df['cluster'] = labels_mapped
    
    # 6. Calcular Silhouette Score
    silhouette = SILHOUETTE_SCORE(X_scaled, labels)
    
    PRINT(f"K={k}, Silhouette={silhouette:.3f}")
    PRINT(f"Distribuci√≥n: Cluster 0 (Bajo): {COUNT(labels_mapped == 0)}, Cluster 1 (Alto): {COUNT(labels_mapped == 1)}")
    
    RETURN df, kmeans, scaler, silhouette


FUNCTION k_sweep(df, k_range=[2, 3, 4, 5, 6, 8, 10]):
    """
    Eval√∫a diferentes valores de K para clustering.
    
    Input: df (DataFrame con features), k_range (lista de K a probar)
    Output: DataFrame con m√©tricas por K
    """
    
    resultados = []
    
    FOR k IN k_range:
        PRINT(f"\nEvaluando K={k}...")
        
        # Clustering
        _, kmeans, scaler, silhouette = clustering_kmeans(df.copy(), k=k)
        
        # Inercia (suma de distancias cuadradas intra-cluster)
        inertia = kmeans.inertia_
        
        resultados.APPEND({
            'k': k,
            'silhouette': silhouette,
            'inertia': inertia
        })
    
    df_ksweep = TO_DATAFRAME(resultados)
    
    # Identificar K √≥ptimo (m√°ximo Silhouette)
    k_optimo = df_ksweep[df_ksweep['silhouette'] == MAX(df_ksweep['silhouette'])]['k'].values[0]
    
    PRINT(f"\n‚úÖ K √≥ptimo: {k_optimo} (Silhouette={MAX(df_ksweep['silhouette']):.3f})")
    
    RETURN df_ksweep, k_optimo


================================================================================
FASE 6: SISTEMA DIFUSO
================================================================================

FUNCTION triangular(x, a, b, c):
    """
    Funci√≥n de membres√≠a triangular.
    
    Input: x (valor), a, b, c (par√°metros del tri√°ngulo)
    Output: Œº(x) ‚àà [0,1]
    """
    
    IF x <= a OR x >= c:
        RETURN 0.0
    ELSE IF a < x < b:
        RETURN (x - a) / (b - a) IF (b - a) > 0 ELSE 0.0
    ELSE:  # b <= x < c
        RETURN (c - x) / (c - b) IF (c - b) > 0 ELSE 0.0


FUNCTION calcular_percentiles_mf(df, features):
    """
    Calcula percentiles para funciones de membres√≠a.
    
    Input: df (DataFrame), features (lista de variables)
    Output: Dict con par√°metros MF por variable y etiqueta
    """
    
    mf_params = {}
    
    FOR feat IN features:
        mf_params[feat] = {}
        
        FOR label IN ['Baja', 'Media', 'Alta']:
            # Obtener percentiles
            percentiles = PERCENTILES_MF[label]
            values = [PERCENTILE(df[feat], p) FOR p IN percentiles]
            
            mf_params[feat][label] = {
                'percentiles': percentiles,
                'values': values  # [a, b, c] para tri√°ngulo
            }
    
    RETURN mf_params


FUNCTION fuzzificar(x, mf_params):
    """
    Calcula membres√≠as para un vector de entrada.
    
    Input: x (Dict con valores de features), mf_params (par√°metros MF)
    Output: Œº (Dict con membres√≠as por feature y etiqueta)
    """
    
    mu = {}
    
    FOR feat IN FEATURES_FUZZY:
        FOR label IN ['Baja', 'Media', 'Alta']:
            a, b, c = mf_params[feat][label]['values']
            mu[f"{feat}_{label}"] = triangular(x[feat], a, b, c)
    
    RETURN mu


FUNCTION activar_reglas(mu):
    """
    Activa reglas difusas (Mamdani: AND = min).
    
    Input: Œº (Dict con membres√≠as)
    Output: w (Dict con activaciones de reglas R1-R5)
    """
    
    w = {}
    
    # R1: Act_Baja ‚àß Sup_Baja ‚Üí Sed_Alto
    w['R1'] = MIN(
        mu['Actividad_relativa_p50_Baja'],
        mu['Superavit_calorico_basal_p50_Baja']
    )
    
    # R2: Act_Alta ‚àß Sup_Alta ‚Üí Sed_Bajo
    w['R2'] = MIN(
        mu['Actividad_relativa_p50_Alta'],
        mu['Superavit_calorico_basal_p50_Alta']
    )
    
    # R3: HRV_Baja ‚àß ŒîCard_Baja ‚Üí Sed_Alto
    w['R3'] = MIN(
        mu['HRV_SDNN_p50_Baja'],
        mu['Delta_cardiaco_p50_Baja']
    )
    
    # R4: Act_Media ‚àß HRV_Media ‚Üí Sed_Medio
    w['R4'] = MIN(
        mu['Actividad_relativa_p50_Media'],
        mu['HRV_SDNN_p50_Media']
    )
    
    # R5: Act_Baja ‚àß Sup_Media ‚Üí Sed_Alto (peso 0.7)
    w['R5'] = MIN(
        mu['Actividad_relativa_p50_Baja'],
        mu['Superavit_calorico_basal_p50_Media']
    ) * 0.7
    
    RETURN w


FUNCTION agregar_consecuentes(w):
    """
    Agrega activaciones por clase de salida.
    
    Input: w (Dict con activaciones de reglas)
    Output: s (Dict con agregaciones por clase)
    """
    
    s = {}
    
    # Sedentarismo Bajo (protecci√≥n)
    s['Sed_Bajo'] = w['R2']
    
    # Sedentarismo Medio (compensaci√≥n)
    s['Sed_Medio'] = w['R4']
    
    # Sedentarismo Alto (riesgo)
    s['Sed_Alto'] = w['R1'] + w['R3'] + w['R5']
    
    RETURN s


FUNCTION defuzzificar(s):
    """
    Defuzzifica usando centroide discreto.
    
    Input: s (Dict con agregaciones)
    Output: score ‚àà [0,1]
    """
    
    # Niveles de salida
    niveles = {
        'Sed_Bajo': 0.2,
        'Sed_Medio': 0.5,
        'Sed_Alto': 0.8
    }
    
    # Calcular centroide
    numerador = SUM([niveles[clase] * s[clase] FOR clase IN s.keys()])
    denominador = SUM(s.values())
    
    IF denominador > 0:
        score = numerador / denominador
    ELSE:
        score = 0.0
    
    RETURN score


FUNCTION binarizar(score, tau=TAU_UMBRAL):
    """
    Convierte score a decisi√≥n binaria.
    
    Input: score ‚àà [0,1], tau (umbral)
    Output: ≈∑ ‚àà {0,1}
    """
    
    RETURN 1 IF score >= tau ELSE 0


FUNCTION sistema_fuzzy_completo(df, mf_params, tau=TAU_UMBRAL):
    """
    Aplica sistema fuzzy completo a todas las semanas.
    
    Input: df (DataFrame), mf_params (par√°metros MF), tau (umbral)
    Output: df con columnas 'Sedentarismo_score' y 'Sedentarismo_crisp'
    """
    
    scores = []
    decisiones = []
    
    FOR i IN RANGE(LEN(df)):
        # Extraer fila
        x = {
            'Actividad_relativa_p50': df.loc[i, 'Actividad_relativa_p50'],
            'Superavit_calorico_basal_p50': df.loc[i, 'Superavit_calorico_basal_p50'],
            'HRV_SDNN_p50': df.loc[i, 'HRV_SDNN_p50'],
            'Delta_cardiaco_p50': df.loc[i, 'Delta_cardiaco_p50']
        }
        
        # Pipeline completo
        mu = fuzzificar(x, mf_params)
        w = activar_reglas(mu)
        s = agregar_consecuentes(w)
        score = defuzzificar(s)
        decision = binarizar(score, tau)
        
        APPEND(scores, score)
        APPEND(decisiones, decision)
    
    df['Sedentarismo_score'] = scores
    df['Sedentarismo_crisp'] = decisiones
    
    RETURN df


================================================================================
FASE 7: VALIDACI√ìN PRIMARIA
================================================================================

FUNCTION calcular_metricas(y_true, y_pred):
    """
    Calcula m√©tricas de clasificaci√≥n.
    
    Input: y_true (array), y_pred (array)
    Output: Dict con m√©tricas
    """
    
    # Matriz de confusi√≥n
    tn, fp, fn, tp = CONFUSION_MATRIX(y_true, y_pred).ravel()
    
    # M√©tricas
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) IF (tp + fp) > 0 ELSE 0
    recall = tp / (tp + fn) IF (tp + fn) > 0 ELSE 0
    f1 = 2 * (precision * recall) / (precision + recall) IF (precision + recall) > 0 ELSE 0
    mcc = MATTHEWS_CORRCOEF(y_true, y_pred)
    
    RETURN {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'mcc': mcc,
        'tn': tn,
        'fp': fp,
        'fn': fn,
        'tp': tp
    }


FUNCTION validar_fuzzy_vs_clusters(df):
    """
    Valida sistema fuzzy contra clusters (verdad operativa).
    
    Input: df (DataFrame con columnas 'cluster' y 'Sedentarismo_crisp')
    Output: Dict con m√©tricas globales y por usuario
    """
    
    # M√©tricas globales
    metricas_global = calcular_metricas(
        df['cluster'].values,
        df['Sedentarismo_crisp'].values
    )
    
    PRINT("\nüìä M√âTRICAS GLOBALES:")
    PRINT(f"  F1-Score: {metricas_global['f1']:.3f}")
    PRINT(f"  Accuracy: {metricas_global['accuracy']:.3f}")
    PRINT(f"  Precision: {metricas_global['precision']:.3f}")
    PRINT(f"  Recall: {metricas_global['recall']:.3f}")
    PRINT(f"  MCC: {metricas_global['mcc']:.3f}")
    
    # M√©tricas por usuario
    metricas_por_usuario = []
    
    FOR usuario_id IN UNIQUE(df['usuario_id']):
        df_user = df[df['usuario_id'] == usuario_id]
        
        metricas = calcular_metricas(
            df_user['cluster'].values,
            df_user['Sedentarismo_crisp'].values
        )
        
        metricas['usuario_id'] = usuario_id
        metricas['n_semanas'] = LEN(df_user)
        
        APPEND(metricas_por_usuario, metricas)
    
    RETURN {
        'global': metricas_global,
        'por_usuario': TO_DATAFRAME(metricas_por_usuario)
    }


================================================================================
FASE 8: AN√ÅLISIS DE ROBUSTEZ
================================================================================

FUNCTION sensibilidad_tau(df, tau_range=[0.20, 0.21, ..., 0.40]):
    """
    Analiza sensibilidad del umbral œÑ.
    
    Input: df (DataFrame con 'cluster' y 'Sedentarismo_score')
    Output: DataFrame con m√©tricas por œÑ
    """
    
    resultados = []
    
    FOR tau IN tau_range:
        # Binarizar con nuevo tau
        y_pred = [1 IF score >= tau ELSE 0 FOR score IN df['Sedentarismo_score']]
        
        # Calcular m√©tricas
        metricas = calcular_metricas(df['cluster'].values, y_pred)
        metricas['tau'] = tau
        
        APPEND(resultados, metricas)
    
    df_sensibilidad = TO_DATAFRAME(resultados)
    
    # Identificar tau √≥ptimo
    tau_optimo = df_sensibilidad[df_sensibilidad['f1'] == MAX(df_sensibilidad['f1'])]['tau'].values[0]
    f1_max = MAX(df_sensibilidad['f1'])
    
    # Rango estable (ŒîF1 < 0.05)
    df_estable = df_sensibilidad[df_sensibilidad['f1'] >= f1_max - 0.05]
    tau_min_estable = MIN(df_estable['tau'])
    tau_max_estable = MAX(df_estable['tau'])
    
    PRINT(f"\nüìä SENSIBILIDAD DE œÑ:")
    PRINT(f"  œÑ √≥ptimo: {tau_optimo:.2f} (F1={f1_max:.3f})")
    PRINT(f"  Rango estable: [{tau_min_estable:.2f}, {tau_max_estable:.2f}]")
    PRINT(f"  Amplitud: {tau_max_estable - tau_min_estable:.2f}")
    
    RETURN df_sensibilidad


FUNCTION leave_one_user_out(df):
    """
    Validaci√≥n Leave-One-User-Out (LOUO).
    
    Input: df (DataFrame completo)
    Output: DataFrame con F1 por usuario test
    """
    
    resultados = []
    
    FOR test_user IN UNIQUE(df['usuario_id']):
        PRINT(f"\nüìä FOLD: test_user = {test_user}")
        
        # Split
        df_train = df[df['usuario_id'] != test_user]
        df_test = df[df['usuario_id'] == test_user]
        
        PRINT(f"  Train: {LEN(df_train)} semanas ({LEN(UNIQUE(df_train['usuario_id']))} usuarios)")
        PRINT(f"  Test: {LEN(df_test)} semanas (1 usuario)")
        
        # Recalcular MF en train
        mf_params_train = calcular_percentiles_mf(df_train, FEATURES_FUZZY)
        
        # Reentrenar clustering en train
        df_train, _, _, _ = clustering_kmeans(df_train.copy(), k=K_CLUSTERS)
        
        # Aplicar fuzzy en train
        df_train = sistema_fuzzy_completo(df_train, mf_params_train, tau=TAU_UMBRAL)
        
        # Optimizar tau en train
        tau_opt = 0.30
        best_f1 = 0.0
        FOR tau IN [0.10, 0.15, 0.20, ..., 0.60]:
            y_pred_train = [1 IF score >= tau ELSE 0 FOR score IN df_train['Sedentarismo_score']]
            f1_train = F1_SCORE(df_train['cluster'], y_pred_train)
            IF f1_train > best_f1:
                best_f1 = f1_train
                tau_opt = tau
        
        PRINT(f"  œÑ √≥ptimo en train: {tau_opt:.2f} (F1={best_f1:.3f})")
        
        # Aplicar clustering a test (mismo scaler)
        # (En implementaci√≥n real, usar scaler entrenado en train)
        df_test, _, _, _ = clustering_kmeans(df_test.copy(), k=K_CLUSTERS)
        
        # Aplicar fuzzy a test (con MF y tau de train)
        df_test = sistema_fuzzy_completo(df_test, mf_params_train, tau=tau_opt)
        
        # Evaluar en test
        metricas_test = calcular_metricas(
            df_test['cluster'].values,
            df_test['Sedentarismo_crisp'].values
        )
        
        metricas_test['test_user'] = test_user
        metricas_test['tau_opt'] = tau_opt
        
        APPEND(resultados, metricas_test)
        
        PRINT(f"  F1 test: {metricas_test['f1']:.3f}")
    
    df_louo = TO_DATAFRAME(resultados)
    
    # Estad√≠sticas globales
    f1_mean = MEAN(df_louo['f1'])
    f1_std = STD(df_louo['f1'])
    
    PRINT(f"\nüìä RESUMEN LOUO:")
    PRINT(f"  F1 promedio: {f1_mean:.3f} ¬± {f1_std:.3f}")
    PRINT(f"  Rango: [{MIN(df_loou['f1']):.3f}, {MAX(df_loou['f1']):.3f}]")
    
    RETURN df_louo


================================================================================
FASE 9: FORMALIZACI√ìN MATEM√ÅTICA
================================================================================

FUNCTION generar_matriz_B():
    """
    Genera matriz B (antecedentes) expl√≠cita.
    
    Output: Matriz B (5 reglas √ó 12 etiquetas)
    """
    
    # Orden de etiquetas: [Act_B, Act_M, Act_A, Sup_B, Sup_M, Sup_A, 
    #                       HRV_B, HRV_M, HRV_A, ŒîC_B, ŒîC_M, ŒîC_A]
    
    B = [
        # R1: Act_B ‚àß Sup_B
        [1, 0, 0,  1, 0, 0,  0, 0, 0,  0, 0, 0],
        
        # R2: Act_A ‚àß Sup_A
        [0, 0, 1,  0, 0, 1,  0, 0, 0,  0, 0, 0],
        
        # R3: HRV_B ‚àß ŒîC_B
        [0, 0, 0,  0, 0, 0,  1, 0, 0,  1, 0, 0],
        
        # R4: Act_M ‚àß HRV_M
        [0, 1, 0,  0, 0, 0,  0, 1, 0,  0, 0, 0],
        
        # R5: Act_B ‚àß Sup_M
        [1, 0, 0,  0, 1, 0,  0, 0, 0,  0, 0, 0]
    ]
    
    RETURN TO_MATRIX(B)


FUNCTION generar_matriz_Cout():
    """
    Genera matriz C_out (consecuentes) expl√≠cita.
    
    Output: Matriz C_out (5 reglas √ó 3 clases)
    """
    
    # Orden de clases: [Sed_Bajo, Sed_Medio, Sed_Alto]
    
    C_out = [
        # R1 ‚Üí Sed_Alto
        [0.0, 0.0, 1.0],
        
        # R2 ‚Üí Sed_Bajo
        [1.0, 0.0, 0.0],
        
        # R3 ‚Üí Sed_Alto
        [0.0, 0.0, 1.0],
        
        # R4 ‚Üí Sed_Medio
        [0.0, 1.0, 0.0],
        
        # R5 ‚Üí Sed_Alto (peso 0.7)
        [0.0, 0.0, 0.7]
    ]
    
    RETURN TO_MATRIX(C_out)


FUNCTION generar_ejemplo_worked_out(df, mf_params, n_semanas=10):
    """
    Genera tabla worked-out con ejemplo detallado.
    
    Input: df (DataFrame), mf_params, n_semanas (n√∫mero de ejemplos)
    Output: DataFrame con fuzzificaci√≥n, activaci√≥n, agregaci√≥n, score
    """
    
    # Seleccionar n_semanas aleatorias
    df_sample = RANDOM_SAMPLE(df, n=n_semanas, random_state=RANDOM_STATE)
    
    ejemplos = []
    
    FOR i IN RANGE(LEN(df_sample)):
        # Datos de entrada
        x = {
            'Actividad_relativa_p50': df_sample.iloc[i]['Actividad_relativa_p50'],
            'Superavit_calorico_basal_p50': df_sample.iloc[i]['Superavit_calorico_basal_p50'],
            'HRV_SDNN_p50': df_sample.iloc[i]['HRV_SDNN_p50'],
            'Delta_cardiaco_p50': df_sample.iloc[i]['Delta_cardiaco_p50']
        }
        
        # Fuzzificaci√≥n
        mu = fuzzificar(x, mf_params)
        
        # Activaci√≥n
        w = activar_reglas(mu)
        
        # Agregaci√≥n
        s = agregar_consecuentes(w)
        
        # Defuzzificaci√≥n
        score = defuzzificar(s)
        
        # Decisi√≥n
        decision = binarizar(score)
        
        # Guardar ejemplo
        ejemplo = {
            'semana': i + 1,
            'usuario_id': df_sample.iloc[i]['usuario_id'],
            **x,  # Agregar valores de entrada
            **{f'mu_{k}': v for k, v in mu.items()},  # Membres√≠as
            **{f'w_{k}': v for k, v in w.items()},  # Activaciones
            **{f's_{k}': v for k, v in s.items()},  # Agregaciones
            'score': score,
            'decision': decision,
            'cluster_real': df_sample.iloc[i]['cluster']
        }
        
        APPEND(ejemplos, ejemplo)
    
    RETURN TO_DATAFRAME(ejemplos)


================================================================================
FASE 10: GENERACI√ìN DE REPORTES
================================================================================

FUNCTION generar_reporte_completo(df, metricas, mf_params):
    """
    Genera reporte Markdown completo.
    
    Input: df, metricas (Dict), mf_params
    Output: String con contenido Markdown
    """
    
    reporte = """
    # INFORME MAESTRO - SISTEMA DIFUSO DE SEDENTARISMO
    
    ## 1. RESUMEN EJECUTIVO
    
    **Objetivo:** Clasificar nivel de sedentarismo semanal usando sistema difuso.
    
    **Datos:** {n_semanas} semanas de {n_usuarios} usuarios.
    
    **M√©todo:** Sistema difuso Mamdani (5 reglas) validado contra clustering K=2.
    
    **Resultados:**
    - F1-Score: {f1:.3f}
    - Accuracy: {acc:.3f}
    - Recall: {rec:.3f}
    
    ## 2. METODOLOG√çA
    
    ### 2.1 Features
    - Actividad_relativa_p50
    - Superavit_calorico_basal_p50
    - HRV_SDNN_p50
    - Delta_cardiaco_p50
    
    ### 2.2 Funciones de Membres√≠a
    {tabla_mf}
    
    ### 2.3 Base de Reglas
    {tabla_reglas}
    
    ## 3. RESULTADOS
    
    ### 3.1 M√©tricas Globales
    {tabla_metricas_global}
    
    ### 3.2 M√©tricas por Usuario
    {tabla_metricas_usuario}
    
    ## 4. AN√ÅLISIS DE ROBUSTEZ
    
    ### 4.1 Sensibilidad de œÑ
    {analisis_tau}
    
    ### 4.2 Leave-One-User-Out
    {analisis_louo}
    
    ## 5. CONCLUSIONES
    
    El sistema difuso desarrollado demuestra:
    - Alta concordancia con clusters (F1=0.84)
    - Robustez al umbral œÑ (rango estable amplio)
    - Generalizaci√≥n a nuevos usuarios (LOUO F1‚âà0.75)
    - Interpretabilidad cl√≠nica (reglas basadas en fisiolog√≠a)
    
    """
    
    # Llenar placeholders
    reporte = reporte.FORMAT(
        n_semanas=LEN(df),
        n_usuarios=df['usuario_id'].nunique(),
        f1=metricas['global']['f1'],
        acc=metricas['global']['accuracy'],
        rec=metricas['global']['recall'],
        tabla_mf=generar_tabla_mf(mf_params),
        tabla_reglas=generar_tabla_reglas(),
        tabla_metricas_global=generar_tabla_metricas(metricas['global']),
        tabla_metricas_usuario=TO_MARKDOWN(metricas['por_usuario']),
        analisis_tau="Ver sensibilidad_tau.csv",
        analisis_louo="Ver louo_summary.csv"
    )
    
    RETURN reporte


================================================================================
MAIN PIPELINE
================================================================================

FUNCTION main():
    """
    Pipeline principal completo.
    """
    
    PRINT("="*80)
    PRINT("PIPELINE COMPLETO - SISTEMA DIFUSO SEDENTARISMO")
    PRINT("="*80)
    
    # ========================================================================
    # FASE 1-2: CARGA Y AGREGACI√ìN
    # ========================================================================
    
    PRINT("\nüì¶ FASE 1-2: CARGA Y AGREGACI√ìN DE DATOS")
    
    datos = cargar_todos_usuarios()
    df_semanal = agregar_todos_usuarios(datos)
    
    PRINT(f"‚úÖ {LEN(df_semanal)} semanas consolidadas")
    
    # ========================================================================
    # FASE 3: FEATURE ENGINEERING
    # ========================================================================
    
    PRINT("\n‚öôÔ∏è FASE 3: FEATURE ENGINEERING")
    
    df_features = calcular_features(df_semanal)
    
    PRINT(f"‚úÖ {LEN(FEATURES_CLUSTER)} features calculadas")
    
    # ========================================================================
    # FASE 4: EDA
    # ========================================================================
    
    PRINT("\nüìä FASE 4: AN√ÅLISIS EXPLORATORIO")
    
    variabilidad = analisis_variabilidad(df_features)
    correlaciones = analisis_correlaciones(df_features)
    missingness = analisis_missingness_acf(df_features)
    
    PRINT("‚úÖ EDA completado")
    
    # ========================================================================
    # FASE 5: CLUSTERING
    # ========================================================================
    
    PRINT("\nüéØ FASE 5: CLUSTERING (VERDAD OPERATIVA)")
    
    df_ksweep, k_optimo = k_sweep(df_features.copy())
    df_clustered, kmeans, scaler, silhouette = clustering_kmeans(df_features.copy(), k=k_optimo)
    
    PRINT(f"‚úÖ Clustering K={k_optimo}, Silhouette={silhouette:.3f}")
    
    # ========================================================================
    # FASE 6: SISTEMA DIFUSO
    # ========================================================================
    
    PRINT("\nüåÄ FASE 6: SISTEMA DIFUSO")
    
    mf_params = calcular_percentiles_mf(df_clustered, FEATURES_FUZZY)
    df_fuzzy = sistema_fuzzy_completo(df_clustered, mf_params, tau=TAU_UMBRAL)
    
    PRINT(f"‚úÖ Sistema difuso aplicado ({LEN(df_fuzzy)} semanas)")
    
    # ========================================================================
    # FASE 7: VALIDACI√ìN PRIMARIA
    # ========================================================================
    
    PRINT("\n‚öñÔ∏è FASE 7: VALIDACI√ìN PRIMARIA")
    
    metricas = validar_fuzzy_vs_clusters(df_fuzzy)
    
    PRINT(f"‚úÖ F1-Score Global: {metricas['global']['f1']:.3f}")
    
    # ========================================================================
    # FASE 8: ROBUSTEZ
    # ========================================================================
    
    PRINT("\nüîç FASE 8: AN√ÅLISIS DE ROBUSTEZ")
    
    df_sens_tau = sensibilidad_tau(df_fuzzy)
    df_louo = leave_one_user_out(df_features.copy())
    
    PRINT("‚úÖ An√°lisis de robustez completado")
    
    # ========================================================================
    # FASE 9: FORMALIZACI√ìN
    # ========================================================================
    
    PRINT("\nüìê FASE 9: FORMALIZACI√ìN MATEM√ÅTICA")
    
    B = generar_matriz_B()
    C_out = generar_matriz_Cout()
    ejemplo_worked_out = generar_ejemplo_worked_out(df_fuzzy, mf_params, n_semanas=10)
    
    # Guardar
    SAVE_CSV(B, "formalizacion_matematica/matriz_B_antecedentes.csv")
    SAVE_CSV(C_out, "formalizacion_matematica/matriz_Cout_consecuentes.csv")
    SAVE_CSV(ejemplo_worked_out, "formalizacion_matematica/ejemplo_worked_out.csv")
    
    PRINT("‚úÖ Formalizaci√≥n completada (6 archivos)")
    
    # ========================================================================
    # FASE 10: REPORTES
    # ========================================================================
    
    PRINT("\nüìÑ FASE 10: GENERACI√ìN DE REPORTES")
    
    reporte = generar_reporte_completo(df_fuzzy, metricas, mf_params)
    SAVE_FILE(reporte, "INFORME_MAESTRO_SISTEMA_DIFUSO_SEDENTARISMO.md")
    
    PRINT("‚úÖ Reportes generados")
    
    # ========================================================================
    # RESUMEN FINAL
    # ========================================================================
    
    PRINT("\n" + "="*80)
    PRINT("‚úÖ PIPELINE COMPLETADO EXITOSAMENTE")
    PRINT("="*80)
    PRINT(f"\nüìä M√âTRICAS FINALES:")
    PRINT(f"  - F1-Score: {metricas['global']['f1']:.3f}")
    PRINT(f"  - Accuracy: {metricas['global']['accuracy']:.3f}")
    PRINT(f"  - Robustez œÑ: ALTO")
    PRINT(f"  - Generalizaci√≥n (LOUO): F1‚âà{MEAN(df_louo['f1']):.3f}")
    PRINT(f"\nüéì LISTO PARA COMIT√â TUTORIAL")
    PRINT("="*80)


# Ejecutar pipeline
IF __name__ == '__main__':
    main()

================================================================================
FIN DEL PSEUDOC√ìDIGO
================================================================================




